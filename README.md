# Library Chatbot ðŸ“šðŸ’¬

[![Python](https://img.shields.io/badge/Python-3.10%2B-blue?logo=python&logoColor=white)](#)
[![React](https://img.shields.io/badge/React-18.2-61dafb?logo=react&logoColor=black)](#)
[![Flask](https://img.shields.io/badge/Flask-Backend-black?logo=flask&logoColor=white)](#)
[![LLM](https://img.shields.io/badge/AI-Ollama%20Local-orange)](#)
[![License](https://img.shields.io/badge/License-MIT-green)](#)

A privacy-focused, AI-powered RAG (Retrieval-Augmented Generation) assistant for university libraries. It answers policy and research questions using local LLMs (via Ollama) by retrieving context from uploaded PDF regulations.

Built with a modular **Git Submodule** architecture, allowing the backend and frontend to be developed and versioned independently.
## Table of Contents
- [Architecture Overview](#architecture-overview)
- [Architecture & UML Diagrams](#architecture--uml-diagrams)
- [Directory Structure](#directory-structure)
- [Configuration Reference](#configuration-reference)
- [RAG Pipeline Workflow](#rag-pipeline-workflow)
- [API Endpoints](#api-endpoints)
- [Key Features](#key-features)
- [Installation](#installation)
- [Versioning](#versioning)
- [Contributing](#contributing)
- [License](#license)

## Architecture Overview
The repository acts as an orchestrator that pins two Git submodules:
- **`chatbot`** â€“ Python/Flask backend implementing LangChain-powered RAG with FAISS and Local LLM integrations.
- **`chatbot_frontend`** â€“ React UI that chats with the backend, streams answers, and renders PDF source popups.
- 
Submodules allow independent iteration of frontend and backend while keeping a single release cadence from the main repo.

### Tech Stack
* **Frontend:** React 18 (Create React App), Axios, React Markdown (for rendering bot responses), Recharts (for analytics).
* **Backend:** Python 3.10+, Flask, LangChain, FAISS.
* **AI/ML:** Ollama (Local LLM), SentenceTransformers (Embeddings).
* **Database:** PostgreSQL.
  

## Architecture & UML Diagrams
### RAG Flow (Sequence)
```mermaid
sequenceDiagram
    participant U as User
    participant F as Frontend (React)
    participant B as Backend (Flask + LangChain)
    participant V as FAISS Vector DB
    participant L as Local LLM (Ollama)

    U->>F: Ask question
    F->>B: POST /chat { question, session_id }
    B->>V: Retrieve relevant chunks
    V-->>B: Top-k context
    B->>L: Prompt with context + question
    L-->>B: Generated answer + citations
    B-->>F: Answer + PDF metadata
    F-->>U: Render message + hoverable PDF popup
```

### Submodule Layout (Graph)
```mermaid
graph TD
    A["library_chatbot (Main Repo)"] --> B["chatbot (Backend Submodule)"]
    A --> C["chatbot_frontend (Frontend Submodule)"]
    style A fill:#9c979c,stroke:#333,stroke-width:2px
```

## Directory Structure
```text
library_chatbot/
â”œâ”€â”€ chatbot/                        # [Submodule] Backend (Flask, RAG, FAISS)
â”‚   â”œâ”€â”€ app/                        # Main application source code
â”‚   â”‚   â”œâ”€â”€ chat/                   # Chat routes and logic
â”‚   â”‚   â””â”€â”€ utils.py                # RAG and utilities
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ pdfs/                   # Directory where uploaded PDFs live
â”‚   â”œâ”€â”€ scripts/                    # Database and FAISS utility scripts
â”‚   â”œâ”€â”€ fine_tuning_data.jsonl      # JSONL data for model fine-tuning
â”‚   â”œâ”€â”€ requirements.txt            # Python dependencies
â”‚   â””â”€â”€ setup_venv.sh               # Backend environment setup script
â”‚
â”œâ”€â”€ chatbot_frontend/               # [Submodule] Frontend Repository
â”‚   â”œâ”€â”€ frontend/                   # Main React Application
â”‚   â”‚   â”œâ”€â”€ public/                 # Static assets
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â””â”€â”€ components/         # UI Components (ChatWidget, PDFPopup)
â”‚   â”‚   â””â”€â”€ package.json            # React dependencies
â”‚   â””â”€â”€ chatbot-widget/             # Standalone widget build (if applicable)
â”‚
â”œâ”€â”€ check_status.sh                 # Script to check health of services
â”œâ”€â”€ start.sh                        # Full installation and startup script
â”œâ”€â”€ start_app.sh                    # Quick startup script for daily use
â””â”€â”€ STARTUP_GUIDE.md                # Detailed manual setup guide
```

## Configuration Reference
| Variable | Description | Default | Required |
| --- | --- | --- | --- |
| `OLLAMA_BASE_URL` | URL where your local Ollama instance is running. | `http://localhost:11434/v1` | âœ… |
| `LLM_MODEL` | The Ollama model to use for generation (e.g., llama3, mistral). | `hf.co/unsloth/Qwen3-4B-Instruct-2507-GGUF:Q8_0` | âœ… |
| `EMBEDDING_MODEL` | Local SentenceTransformer model for vectorizing PDFs. | `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2` | âœ… |
| `VECTOR_STORE_PATH` | Filesystem path for FAISS index persistence. | `./vectorstore` | â¬œ |
| `CHUNK_SIZE` | Document chunk size (tokens/characters) during ingestion. | `500` | â¬œ |
| `CHUNK_OVERLAP` | Overlap between chunks to preserve context. | `50` | â¬œ |
| `ALLOW_FILE_TYPES` | Comma-separated whitelist of uploadable formats (e.g., `pdf`). | `pdf` | â¬œ |
| `MAX_FILE_SIZE_MB` | Maximum upload size per document. | `10` | â¬œ |
| `PDF_FOLDER_PATH` | Directory where PDFs are stored for download. | `./data/pdfs` | âœ… |
| `DATABASE_URL` | PostgreSQL connection string.. | `postgresql://postgres@localhost/hu_chatbot2` | âœ… |
| `FRONTEND_URL` | Allowed origin for CORS between frontend and backend. | `http://localhost:3000` | âœ… |


## RAG Pipeline Workflow
1.  **Ingestion:** PDFs are manually placed into the `data/pdfs` folder and processed/embedded using `sentence-transformers`.
2.  **Indexing:** FAISS stores the vector embeddings locally at `VECTOR_STORE_PATH` for fast retrieval.
3.  **Retrieval:** When a question arrives, the backend queries FAISS to find the most relevant document chunks via cosine similarity.
4.  **Generation:** The retrieved context and user prompt are sent to the **local LLM (via Ollama)** to generate a grounded response.
5.  **Smart Citation:** The backend cleans metadata and maps the answer to specific PDF pages.
6.  **Delivery:** The final answer and source metadata are returned as a JSON response to the Frontend, which renders the answer and the interactive **PDF Popup**.

## API Endpoints
- `POST /chat` â€“ Main chat endpoint. Parameters: `question`, `session_id` (body JSON). Returns answer + source metadata.
- `GET /download/<filename>` â€“ Download a stored PDF referenced by citations.

## Key Features
- **RAG Powered**: Combines LangChain, FAISS, and Local LLMs (Ollama) to ground answers in uploaded PDFs.
- **Smart Source Citation (v0.2.0)**: Hoverable blue PDF icon reveals a polished card with filename, page number, and a download button.
- **Modular via Git Submodules**: Backend and frontend evolve independently while the main repo manages versions.
- **Developer-Friendly Scripts**: `start.sh` (full setup), `start_app.sh` (quick start), and `check_status.sh` (runtime diagnostics).
- **Admin Dashboard**: Built-in analytics to track chat stats, review user feedback, and manage the knowledge base.

## Installation
1. **Clone with submodules**
   **Option A: Using HTTPS (Recommended for most users)**
    ```bash
    git clone --recurse-submodules https://github.com/Mehmetmrtgl/library_chatbot.git
    cd library_chatbot
    ```
   **Option B: Using SSH (For developers with SSH keys)**
    ```bash
    git clone --recurse-submodules git@github.com:Mehmetmrtgl/library_chatbot.git
    cd library_chatbot
    ```
2. **First-time setup (recommended)** â€“ installs Python deps, creates DB, installs npm packages, and starts everything:
   ```bash
   ./start.sh
   ```
3. **Subsequent runs** â€“ reuse existing environments and start services quickly:
   ```bash
   ./start_app.sh
   ```
4. **Check component status** (optional):
   ```bash
   ./check_status.sh
   ```

> Both startup scripts expect Ollama (for local models) to be available at `http://localhost:11434`. See `STARTUP_GUIDE.md` for manual steps and troubleshooting.


## Versioning
Semantic Versioning; current release: **v0.2.0** (introduces the smart PDF citation popup).

## Contributing
We welcome contributions! Please follow the workflow below to ensure a smooth collaboration.

### Important Rule
**Direct Pull Requests to the `main` branch are restricted.**
External contributors must target the **`dev`** branch. The `main` branch is reserved for stable releases only.

### Workflow

1.  **Fork & Clone**
    Fork the repository and clone it locally with submodules:
    ```bash
    git clone --recurse-submodules https://github.com/Mehmetmrtgl/library_chatbot.git
    cd library_chatbot
    ```

2.  **Check/Create `dev` Branch**
    Since this project is new, the `dev` branch might not exist yet.
    * **If `dev` exists:** Checkout to it.
    * **If `dev` DOES NOT exist (You are the first contributor):** Create it from `main`.

     
    ```bash
    # Check remote branches
    git branch -r 

    # If origin/dev exists:
    git checkout dev

    # If origin/dev DOES NOT exist (First time setup):
    git checkout -b dev main
    ```

3.  **Create Your Feature Branch**
    Always create your feature branch off `dev`:
    ```bash
    git checkout -b feature/amazing-feature dev
    ```

4.  **Develop & Commit**
    * Make changes in submodules (`chatbot` or `chatbot_frontend`).
    * Commit submodule changes first, then the main repo.
    ```bash
    git add .
    git commit -m "feat: added new amazing feature"
    ```

5.  **Open a Pull Request**
    * Push your branch to your fork.
    * Open a Pull Request targeting the **`dev`** branch of this repository.
    * *(Note: If `dev` doesn't exist on the main repo yet, please mention in your PR that it should be created).*

### Git Flow Visualization
```mermaid
sequenceDiagram
    autonumber
    participant Contributor
    participant Fork as Your Fork
    participant Main as Main Repo

    Note over Contributor, Main: Phase 1: Setup
    Contributor->>Main: Fork repository on GitHub
    Contributor->>Contributor: git clone --recursive <fork-url>
    Contributor->>Contributor: git checkout -b my-feature dev

    Note over Contributor: Phase 2: Work
    Contributor->>Contributor: Update submodules & Write code
    Contributor->>Contributor: git commit -m "feat: add new feature"

    Note over Contributor, Main: Phase 3: Submit
    Contributor->>Fork: git push origin my-feature
    Fork->>Main: Open Pull Request (Target: dev branch)
```

## License
This project is licensed under the MIT License - see the LICENSE file for details.
